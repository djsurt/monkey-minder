\documentclass[journal,twocolumn,american]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage[pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{calc}
\renewcommand{\ttdefault}{cmtt}

% via https://stackoverflow.com/a/123184/8762161
\newlength{\mmdfigureWidthTmp}

\newcommand{\mmdfigure}[2]{\begin{figure}[h]%
    % get the width of the figure unscaled %
    \settowidth{\mmdfigureWidthTmp}{\includegraphics{#1.mmd.pdf}}%
    % give the diagrams a constant scale down regardless of their size. %
    % (based on a rough conversion between their font sizes, assuming i'm correct about those. -amgg) %
    \setlength{\mmdfigureWidthTmp}{\mmdfigureWidthTmp*10/16}%
    % scale that down further but only if necissary to not exceed text width %
    % (note that we use columnwidth here, not textwidth, since we're in a 2-column layout) %
    \setlength{\mmdfigureWidthTmp}{\minof{\mmdfigureWidthTmp}{\columnwidth}}%
    \centering%
    \includegraphics[width=\mmdfigureWidthTmp]{#1.mmd.pdf}%
    \caption{#2}%
\end{figure}%
}

\begin{document}

% title stuff
\title{Project Title}
\author{%
Caleb~Fringer~(0123456789,~caleb.fringer@sjsu.edu)\\%
Adrian~Guerra~(0123456789,~adrian.guerra@sjsu.edu)\\%
Dhananjay~Surti~(0123456789,~dhananjay.surti@sjsu.edu)%
}
\maketitle


\section{Prior Art}
Zookeeper achieves consensus through the Zookeeper Atomic Broadcast (Zab)
protocol, which is a leader-based atomic broadcast system designed to ensure
consistent state replication across nodes [1]. Zab shares some conceptual similarities
with the Raft algorithm, such as leader election and log replication, but was developed
independently [3]. Since Raft has become widely adopted and is designed to be
understandable, we will elect to use Raft to provide both leader election and
consistency in our implementation.
Much research has been done to improve the performance and consistency of Raft
under failure scenarios. For instance, in [4] Kim et al. explored improving leader-based
consensus algorithms such as Raft using federated learning to enhance stability and
performance. Furthermore, Liu et. al has suggested improvements in both performance
and strict consistency guarantees over the original Raft algorithm by requiring strong
consistency of followers logs [5]. They show that their accelerated log backtracking Raft
algorithm can reduce the number of RPCs required to resolve log conflicts under
multiple leader election failure scenarios [5]. Similarly, Liu et Al. (2023) propose an
optimized Raft algorithm (SS-Raft) that incorporates snapshot-based log compression
to reduce excessive log growth during replication and improve recovery under unreliable
networks and churn failures [6]. Finally, Paznikov et. al proposes a leaderless approach
to atomic broadcast using the actor model of concurrency [7]; while the actor model
approach is appealing to us, we think designing a ZooKeeper service around the actor
model is a more appropriate topic for subsequent research projects.
Building on these ideas, our project will use Raft atomic broadcast to achieve strong
consistency among replicas. In particular, we draw inspiration from recent research
such as [8], which demonstrates that atomic broadcast can be achieved efficiently
through a reduction from multivalued Byzantine Agreement (MBA) while avoiding heavy
cryptographic signatures, and from [9] S. Mane et. al which proposes additions to
ZooKeeper which give it better resilience against Byzantine faults.

\section{Goals and Motivation}
The goal of our project is to implement a distributed tree-based data store similar
to Apache ZooKeeper using Golang. Like ZooKeeper, our service guarantees linearizable 
writes while supporting read-heavy workloads by guaranteeing montonic reads.
Developing such a service involves implementing protocols for leader election, 
replication & consistency, and Lamport timestamps to provide causal ordering.
The service must be fault-tolerant, robust to network partitions, and available to 
many concurrent clients.

For our implementation, we wanted to adhere to a few additional constraints. First,
we chose to use Golang as our programming environment. Golang adopts the
Communicating Sequential Processes (CSP) concurrency model, and we chose to fully
embrace this paradigm in our implementation by avoiding locks, semaphores, and other
concurrency primitives. Instead, we relied almost exclusively on Go's channel primitive
for managing concurrent access to shared resources.

Second, instead of Zab we chose to utilize the Raft consensus algorithm. While
Zab and Raft are both equivalent to Paxos (**CITE**), Raft is more recent algorithm
and used in popular services such as etcd and MongoDB (**CITE**). Additionally, 
Raft claims to be a more understandable alternative to Paxos. This required us 
to relax the guarantees of the Raft protocol, which requires total ordering of 
both reads and writes.

Third, we chose to use gRPC and Protocol buffers as our transport layer. gRPC is
an RPC framework created by Google (**CITE**), and Protobufs are Google's language 
independent serialization framework (**CITE**). We chose these tools due to their
popularity in the industry and our desire to become acquainted with tools that
we may encounter later in our careers.

By building our project within these parameters, we hoped to gain practical
experience navigating the unique challenges posed by distributed systems.

\section{Challenges Addressed}
\lipsum[1]

\section{Design}
Our design takes into account the distributed systems challenges of heterogeneity,
openness, security, failure handling, concurrency, quality of service and scalability.

\textbf{Heterogeneity}:
We expose out service via gRPC with Protocol Buffers, which provides a language-agnostic, 
platform-agnostic interface for clients to interact between clients and servers. The Raft
server implements the \texttt{RaftServer} gRPC service and uses generated stubs on both the 
server and client side, allowing future clients to be written in any language supported by
gRPC without changing the core consensus implementation.[Have some sort of citation maybe].

\textbf{Openness}:
The system is designed as a set of composable modules: the replicated log (\texttt{Log}), the
tree-based data store (\texttt{Tree}), and the Raft consensus layer (\texttt{RaftServer}). The
log is generic over the entry snapshot types and only depends on an abstract \texttt{Snapshot}
interface, so other state machines (besides our ZooKeeper-like tree) can be plugged in 
without changing the Raft code.

\textbf{Security}:
At the transport layer we use gRPC, which supports mutual TLS for channel encryption and
authentication. In the current prototype, we use insecure credentials for simplicity, but
the design isolates transport setup inside \texttt{connectToPeers}, so adding TLS in the future
will be straightforward.

\textbf{Failure Handling}:
Raft provides deterministic leader election and log replication under crash failures.
The follower nodes use randomized election timeouts to avoid split votes, and step up to
the candidate role when they stop hearing from a leader. Candidates send \texttt{RequestVote}
RPCs in parallel and step own when they see a higher term, while leaders periodically
send empty \texttt{AppendEntries} (heartbeats) and perform log backtracking and truncation when
followers' logs diverge.

\textbf{Concurrency}:
Each node runs a single Raft loop(\texttt{doLoop}) that dispatches into role-specific
loops (\texttt{doFollower}, \texttt{doCandidate}, \texttt{doLeader}). These loops serialize
all consensus state transitions. RPC handlers for \texttt{RequestVote} and \texttt{AppendEntries}
only enqueue requests on channels and block waiting for responses, which keeps Raft state
changes confined to the state machine goroutine. Parallelism is used only for outbound RPCs
(e.g, parallel vote requests and AppendEntries to followers), ensuring a simple, mostly single-threaded
correctness story while still utilizing concurrency for I/O.

\textbf{Quality of Service}:
Randomized election timeouts limit unecessary split vites, while periodic heartbeats prevent
unnecessary elections during normal operation. The log module supports compaction (squashing)
and snapshotting so that old entries can be removed once safely replicated and applied, which
bounds memory usage and keeps replication latency low during long-running workloads.

\textbf{Scalability}:
The system scales horizontally by adding more Raft servers to the cluster. Clients talk to
any node in the cluster(TODO: MIGHT NEED TO CHANGE THIS). Reads and writes are internally forwarded
to the current leader, which serializes updates and replicates them to followers using AppendEntries.
Because the replicated log is generic, different state machines can be plugged in to support different
workloads without changing the consensus layer.

\textbf{Transparency}:
Clients interact with the system as if it were a single, ZooKeeper-like service with a heirarchical
namespace. They are don't know which node is currently the leader, when failovers occur, or how many replicas
exist in the cluster. Leader elecion, log replication, and commit-index management are all hidden behind the
API, providing failure transparency and location transparency.

\subsection{Key Components and Algorithms}

\subsubsection{RaftServer and Node Roles}
The \texttt{RaftServer} type represents a single Raft node. It maintains the node identity (\texttt{Id}), peer addresses,
current role (\texttt{FOLLOWER}, \texttt{CANDIDATE}, \texttt{LEADER}), current term, the node it voted for,
gRPC server state, a replicated log, and per-RPC channels for AppendEntries and RequestVote. 

The lifecycle is driven by the \texttt{doLoop}, which dispatches to state-specific loops:
\begin{itemize}
    \item \texttt{doFollower}: waits for AppendEntries and RequestVote requests,
    resetting its election timer on each valid interaction. If the time expires, the node
    promotes itself to \texttt{CANDIDATE}.
    \item \texttt{doCandidate}: increments its term, votes for itself, and sends parallel
    RequestVote RPCs to all peers. It becomes \texttt{LEADER} upon winning a majority,
    reverts to \texttt{FOLLOWER} if it sees a higher term, or restarts the election on timeout.
    \item \texttt{doLeader}: manages per-follower replication pointers
    (\texttt{nextIndex} and \texttt{matchIndex}), sends AppendEntries heartbearts and log entries,
    and responds to vote and AppendEntries RPCs from other nodes, stepping down when 
    observing a higher term.
\end{itemize}

\subsubsection{Log and Snapshot Module}
The replicated log maintains:
(i) a \texttt{headSnapshot} representing the state up to the first stored entry,
(ii) a \texttt{tailSnapshot} that is updated as new entries are appended,
(iii) an index offset for the first logical entry,
(iv) the slice of entries, and
(v) a commit index.

Key operations include:
\begin{itemize}
    \item \texttt{Append}: add a new, uncommitted log entry.
    \item \texttt{Commit}: advance the commit index and apply all entries up to 
    that index to the tail snapshot, updating the state machine/
    \item \texttt{GetEntryAt}, \texttt{GetEntryLatest}: inspect specific entries
    or the most recent entry, used for Raft log-matching and vote comparisons.
    \item \texttt{SquashFirstN} and \texttt{SquashUntil}: compact the log by merging
    old entries into the head snapshot, freeing memory.
    \item \texttt{TruncateAt}: delete uncommitted suffixes when reconciling divergent log with a new leader.
\end{itemize}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=2.5in]{test-uml.mmd.pdf}
%     \caption{Hello world.}
% \end{figure}

\mmdfigure{test-uml}{Hello world.}

\lipsum[2]

\subsection{Log}
\mmdfigure{log-diagram}{Log UML}
\mmdfigure{log-diagram-structure}{Log structure}

\lipsum[3]

\section{Evaluation}
\lipsum[1]

\section{Implementation Details}
Our implementation is divided into two main layers: the Client API Layer and the Consensus Layer.

\subsection{Client API Layer}
The client layer provides several endpoints for interacting with the system. These include operations for creating, deleting, and checking the existence of paths, as well as retrieving and setting data. Additionally, clients can retrieve children of a path and synchronize their state with the server.

\subsection{Consensus Layer}
The consensus layer is built on the Raft protocol, replacing Zookeeper's Zab protocol. It operates with three distinct node states: Follower, Leader, and Candidate.
Each node runs a state machine that delegates tasks to the appropriate handler based on its current role in the cluster.

\subsubsection{Process Models and Types}
Our Raft node is implemented by \texttt{ElectionServer}, which contains the node identity (\texttt{Id}), transport configuration (\texttt{Port}, \texttt{peers}),
local state (\texttt{state}) $\in$ \{\texttt{FOLLOWER}, \texttt{CANDIDATE}, \texttt{LEADER}\}, and protocol variables (\texttt{term} and \texttt{logIndex}). We define
strongly-types aliases for clairty:
\texttt{Term} and \texttt{LogIndex} (\texttt{uint64}), and \texttt{NodeId} (\texttt{uint64}).

The server exposes the Raft RPC through gRPC (service \texttt{ElectionServer}).
Inbound RPCs are separated into internal channels:
\texttt{aeRequestChan/aeResponseChan} for AppendEntries and \texttt{rvRequestChan/rvResponseChan} for RequestVote.

\subsubsection{Lifecycle and Event Loop}
The method \texttt{Serve()} performs three responsibilities:
(i) create a TCP listener on \texttt{localhost:\$\{Port\}},
(ii) construct and register a gRPC server that binds the Raft RPC handlers,
and (iii) spawns the node's control loop \texttt{doLoop(ctx)} which drives the Raft state machine.

\texttt{doLoop} runs forever, redirecting to one of \texttt{doFollower}, \texttt{doCandidate}, or 
\texttt{doLeader} based on \texttt{state}. Each state handler is a blocking loop selecting over protocol
events (RPCs) and timers, and exits only when a state transition occurs (e.g. timeout, quorum win, or term regression). This makes sure 
that only one state handler is active at a time.

\subsubsection{Timers and Failure Detector (Raft \S5.2)}
We implement a randomized election timeout using
\texttt{getNewElectionTimer()}, which draws uniformly from
$[1500,2000]\,$ms. The node resets this timer on any leader activity
(a valid AppendEntries) or on serving a RequestVote, matching the “leader
stickiness” property of Raft. The heartbeat period is set to
\texttt{DEFAULT\_HEARTBEAT\_TIMEOUT}$=750\,$ms; leaders refresh this
periodically to prevent follower timeouts.

\subsubsection{RPC Handler Boundary}
Inbound RPCs (\texttt{AppendEntries}, \texttt{RequestVote}) are thin on purpose: they push the 
immutable request onto the corresponding request channel and block on the response channel. The state
machine consumes these requests inside the current role's loop, generates the reply, and sends it back.
This preserves a single point of serialization for correctness and simplifies concurrency reasoning
(network threads never mutate the Raft state directly).

\subsubsection{Term Management and Common Logic (\S5.1)}
Both RPC paths share "common" characteristics implemented in \texttt{doCommonAE} and \texttt{doCommonRV}:

\begin{itemize}
    \item \textbf{Monotonic terms}: If a request carries a term $T > $ \texttt{s.term},
    the server marks \emph{staleTerm/shouldAbdicate} and defers \texttt{s.term = T} until
    the response is composed; the caller (state loop) then transitions to \texttt{FOLLOWER}
    as needed. Replies always include the server's current term.

    \item \textbf{Reject stale terms}: If $T < \texttt{s.term}$ the server rejects.
\end{itemize}

This shared handlign ensures both RPC types correctly implement Raft's term
invariant regardless of current role.

\subsubsection{AppendEntries Path (Follower \& Leader, \S5.3)}
\texttt{doCommonAE} currently implements:
(i) term checks per \S5.1,
(ii) placeholder return of \texttt{Success=true} for non-conflicting cases.
The log-consistency checks mandated by \S5.3 are marked as TODOs:
\begin{enumerate}
  \item Verify that the receiver’s log at \texttt{PrevLogIndex} has term \texttt{PrevLogTerm};
  otherwise reply \texttt{Success=false}.
  \item On conflict (same index, different term), delete the conflicting entry and all that follow.
  \item Append new entries not already present; advance commitIndex if appropriate.
\end{enumerate}
While omitted for brevity in this iteration, the control flow and state transitions
are structured to accommodate these steps.

\subsubsection{RequestVote Path (Follower \& Candidate, \S5.2, \S5.4)}
\texttt{doCommonRV} enforces:
\begin{enumerate}
    \item \textbf{Term rule}: adopt higher term and abdicate to follower if needed.
    \item \textbf{Vote once per term}: grant vote only if \texttt{votedFor} is null or matches \texttt{CandidateId}.
    \item \textbf{Log up-to-dateness} (\S5.4): grant vote only if candidate’s log is at least as up-to-date as receiver’s log.
    We compute this through \texttt{LastLog.AtLeastUpToDateAs}, which first compares terms, the indices.
\end{enumerate}
The function returns both the vote and whether a higher term was observed, so callers can reset
\texttt{votedFor} appropriately.

\subsubsection{Follower Role}
\texttt{doFollower} maintains the election time and processes two classes of events:
\begin{enumerate}
    \item \textbf{AppendEntries}: processes through \texttt{doCommonAE}. On success or on seeing
    higher term, resets election timer. If the leader's term is newer, clear \texttt{votedFor}
    \item \textbf{RequestVote}: process via \texttt{doCommonRV}. Reply and reset the timer.
\end{enumerate}
If the timer elapses without observed leader activity, node transitions to \texttt{CANDIDATE}.

\subsubsection{Candidate Role}
\texttt{doCandidate} increments \texttt{term} and self-votes, then issues parallel  \texttt{RequestVote}
RPCs vis \texttt{requestVotes(ctx)}.
Votes are aggregated on a channel of \texttt{VoteResult}. The candidate:
\begin{itemize}
    \item Becomes \texttt{LEADER} upon majority (strictly more than half of $N$).
    \item Reverts to \texttt{FOLLOWER} if any response carries a strictly higher term.
    \item Restarts the election with a fresh randomized timeout if it fails to gain a quorum.
    \item Rejects external vote requests for the current term because it already voted for itself,
            unless a higher term appears (abdication).
\end{itemize}

\subsubsection{Leader Role}
Upon entering \texttt{doLeader}, the node immediately asserts leadership by broadcasting
\textit{heartbeats} (AppendEntries with empty \texttt{Entires}) to all peers. A ticker
(\texttt{DEFAULT\_HEARTBEAT\_TIMEOUT}) triggers additional heartbeats periodically. Any
inbound AppendEntries with a higher term or RequestVote with a higher term causes abdication
to \texttt{FOLLOWER}. A short debug timer bounds the demo session for observability.

% --- INSERT DIAGRAMS HERE ---
\mmdfigure{raft-election}{Raft Leader Election State Machine}
\mmdfigure{raft-rpc-flow}{Raft RPC Interaction During Election}
% --- END INSERT ---

\subsubsection{Outbound RPCs and Parallelism}
\texttt{sendHeartbeats} constructs a single immutable heartbeat request and invokes
\texttt{AppendEntries} on each \texttt{ElectionClient} concurrently. Results are pushed to a
shared \texttt{responses} channel.

\texttt{requestVotes} similarly fans out \texttt{RequestVote} calls. Results are wrapped in
\texttt{VoteResult} (peer id, grant bit, observed term, error) and gathered on a buffered channel.
A \texttt{WaitGroup} coordinates producer completion, after which the channel is closed.

\subsection{Log Module}
The \texttt{log} package procides a lightweight, generic implementation of Raft's replicated log abstraction.
It maintains an ordered sequence of log entries and two associated snapshots representing the state of the replicated state machine.

\subsubsection{Design Overview}
The core type \texttt{Log[E, S]} is a parameter over:
\begin{itemize}
    \item \textbf{E}: the entry type (application-specific commands).
    \item \textbf{S}: a type implementing the \texttt{Snapshot[E, S]} interface, representing the materialized state after applying entries.
\end{itemize}

Each log instance tracks:
\begin{itemize}
    \item \texttt{headSnapshot}: a snapshot representing the state up to the first entry in the log.
    \item \texttt{tailSnapshot}: a snapshot continously updates as new entries are appended.
    \item \texttt{realFirstIndex}: the global index of the first log entry after squashing or compaction.
    \item \texttt{entries}: an in-memory slice of remaining unapplied entries.
\end{itemize}

\subsubsection{Snapshot Interface}
The \texttt{Snapshot} interface defines two generic methods:
\begin{verbatim}
    ApplyEntry(Entry)
    Clone() Self
\end{verbatim}
\textbf{ApplyEntry} mutates the snapshot by applying an entry's effects, 
while \textbf{Clone} ensures that new logs or truncations can safely copy snapshot state.

\subsection{Log Initialization}
\texttt{NewLog(initialSnapshot, indexOffset)} creates a new log from an existing snapshot.
The constructor clones the snapshot for the head (persistent state) and it reuses it as the tail (mutable state).
The \texttt{realFirstIndex} tracks the absolute position of the first entry, allowing the log to appear contiguous across truncations.

\subsubsection{Appending Entries}
\texttt{Append(entry)} performs two actions atomically:
\begin{enumerate}
    \item Applies the new entry to the tail snapshot via \texttt{ApplyEntry}, keeping the in-memory snapshot consistent.
    \iitem Appends the entry to the \texttt{entries} slice.
\end{enumerate}
This ensures O(1) amortized append performance while keeping the latest snapshot always consistent with all entries applied so far.

\subsubsection(Snapshotting and Compaction)
To avoid excessive growth of the log, the Snapshot method supports selective compaction:
\begin{itemize}
    \item \texttt{SquashFirstN(n)} applies and removes the first \texttt{n} entries from the log, merging them into \texttt{headSnapshot} and incrementing \texttt{realFirstIndex}.
    \item \texttt{SquashUntil(predicate)} squashes all entries up to (but not including) the first entry matching the given predicate.
\end{itemize}
This method follow's Raft's snapshotting behavior (\S7 of the Raft paper) without persistent storage: once entries are safely replicated and applied by all follower, they can be compacted.


\subsection{Transport and Peer Topology}
\texttt{connectToPeers} creates long-lived gRPC client stubs
(\texttt{raftpb.ElectionClient}) to all peers listed in \texttt{peers: map[NodeId]string}.
These stubs are stored in \texttt{s.peerConns}. 
Derp.

\section{Demonstration}
\lipsum[1]


\section{Performance}
\lipsum[1]

\section{Testing}

To validate linearizability, monotonic reads, cross-node consistency, and failure recovery,
we designed a set of test cases across 3 to 5 node clusers. Each test is described with its setup, actions, observations,
and expected results.

\subsection{Linearizable Writes}

\subsubsection{Test 1: Single-Leader Sequential Create/Update}
\textbf{Setup:} Three-node cluster. A Leader $L$ is elected. A client $C_1$ is connected to $L$.

\textbf{Actions:}
\begin{enumerate}
    \item $C_1$: \texttt{Create("/foo", "v1")}
    \item $C_1$: \texttt{SetData("/foo", "v2", -1)}
    \item $C_1$: \texttt{SetData("/foo", "v3", -1)}
    \item After each write, $C_1$ immediately issues \texttt{GetData("/foo")}.
\end{enumerate}
\textbf{Expected Result:} Reads return \emph{v1}, then \emph{v2}, then \emph{v3} in strict order.
No reordering, skipping or stale values.

\subsubsection{Test 2: Concurrent Writes Ordered by Leader}
\textbf{Setup:} Three-node cluster. Clients $C_1$ and $C_2$ connected to Leader $L$.
\textbf{Actions:} $C_1$ and $C_2$ concurrently issue \texttt{SetData("/bar", "A", -1)}
and \texttt{SetData("/bar", "B", -1)}.
\textbf{Observation:} After success, all clients read \texttt{GetData("/bar")} on $C_1$,
$C_2$, and follower $F$.
\textbf{Expected Result:} All nodes converge on a single final value in the leader's chosen
serialization order (either $A \rightarrow B$ or $B \rightarrow A$).
No interleaving or non-linearizable outcomes.

\subsection{Monotonic Reads}

\subsubsection{Test 3: Read-Only Client Monotonicity}
\textbf{Setup:} Client $C_3$ reads from $F$. Leader $L$ performs updates.
\textbf{Actions:}
$L$: \texttt{SetData("/m", "1")} then \texttt{SetData("/m", "2")}.
$C_3$ polls \texttt{GetData("/m)} every 100ms.
\textbf{Expected Result:} Observed values are monotonic: once $C_3$ sees ``2'', it never
returns ``1'' again. Temporary lag is allowed but regressions are not.

\subsubsection{Test 4: Monotonic Reads Across Session Migration}
\textbf{Setup:} Client $C_4$ initially connected to follower $F_1$. Migrates mid-run to $F_2$.
\textbf{Actions:}
$L$: \texttt{SetData("/n", "x")} then \texttt{SetData("/n", "y")}.
$C_4$ reads \texttt{GetData("/n")} from $F_1$, migrates, then continues reading on $F_2$.
\textbf{Expected Result:} No backwards reads: after observing ``y'' on $F_1$, reads on 
$F_2$ never return ``x''. Followers must not serve ilder states than those already observed.

\subsection{Cross-Node Consistency}
\subsubsection{Test 5: Writes on Leader, Reads on All Nodes}

\textbf{Setup:} Five-node cluster. Clients $C_1\dots C5$ connected to different nodes.

\textbf{Actions:} Leader performs:
\begin{enumerate}
    \item \texttt{Create("/k"),"alpha"}.
    \item \texttt{SetData("/k", "beta")}
\end{enumerate}

\textbf{Observation:} All clients repeatedly read \texttt{GetData("/k")} for 5 seconds.

\textbf{Expected Result:} All nodes converge to ``beta''. No node returns a value outside the committed sequence.
Temporary follower lag is acceptable.

\subsubsection{Failure Recovery}
\subsubsection{Test 6: Leader Crash After Append, Before Commit}

\textbf{Setup:} Three-node cluster. Followers $F_1$, $F_2$.

\textbf{Actions:}
Leader $L$ appends \texttt{SetData("/r", "new")} then crashes \emph{before} receiving a quorum ACK.

\textbf{Observation:} Reads on $F_1$ and $F_2$ via \texttt{GetData("/r)}.

\textbf{Expected Result:} Value remains the old value. The uncommitted write is discarded after new leader election. No 
partial visibility.

\subsection{Quorum and Availability}
\subsubsection{Test 7: Minority Partition Unavailable for Writes}

\textbf{Setup:} Five-node cluster partitioned into 3-node majority and 2-node minority.  

\textbf{Actions:} Clients attempt writes in both partitions.  

\textbf{Expected Result:}  
\begin{itemize}
  \item Majority partition continues serving writes normally.
  \item Minority partition rejects or blocks writes.
  \item Minority reads may lag or fail.
  \item After healing, minority catches up from leader log.
\end{itemize}

\subsubsection{Test 8: Node Join and Catch-Up}

\textbf{Setup:} Three nodes perform 100 sequential updates on \texttt{/z}.

\textbf{Actions:} Add new node $N_4$, allow it to synchronize.

\textbf{Expected Result:} After catch-up, $N_4$ returns the latest committed value.
Log backfull converges without divergence.

\subsection{Watches and Notifications}
\subsubsection{Test 9: Watch Triggers on Immediate Children}

\textbf{Setup:} Client registers a watch via \texttt{GetChildren("/p", watchChan)}.

\textbf{Actions:}
Leader creates \texttt{/p/a}, then \texttt{/p/a/b}, then deletes \texttt{/p}.

\textbf{Expected Result:}
Notifications fire for \texttt{/p/a} (child created) and \texttt{/p} (deleted).
No notification for grandchild \texttt{/p/a/b}, consistent with Zookeeper semantics.

\subsection{Operational Parameters}
\begin{itemize}
    \item Cluster sizes tested: 3-node and 5-node.
    \item Timing: heartbeat interval and election timeout ranges.
    \item Failure modes: crash-stop, network partitions.
    \item Client placement: leader-connected vs. \ follower-connected vs. \ migrating.
    \item Metrics recorded: commit index progression, read sequences, convergence time, 
    leadership transitions.
\end{itemize}


\section{Conclusions}
\lipsum[1]


\end{document}

