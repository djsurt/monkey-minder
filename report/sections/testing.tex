To validate linearizability, monotonic reads, cross-node consistency, and failure recovery,
we designed a set of test cases across 3 to 5 node clusers. Each test is described with its setup, actions, observations,
and expected results.

\subsection{Linearizable Writes: Single-Leader Sequential Create/Update}
\textbf{Setup:} Three-node cluster. A Leader $L$ is elected. A client $C_1$ is connected to $L$.

\textbf{Actions:}
\begin{enumerate}
    \item $C_1$: \texttt{Create("/foo", "v1")}
    \item $C_1$: \texttt{SetData("/foo", "v2", -1)}
    \item $C_1$: \texttt{SetData("/foo", "v3", -1)}
    \item After each write, $C_1$ immediately issues \texttt{GetData("/foo")}.
\end{enumerate}

\textbf{Expected Result:} Reads return \emph{v1}, then \emph{v2}, then \emph{v3} in strict order.
No reordering, skipping or stale values.

\subsection{Linearizable Writes: Concurrent Writes Ordered by Leader}

\textbf{Setup:} Three-node cluster. Clients $C_1$ and $C_2$ connected to Leader $L$.

\textbf{Actions:} $C_1$ and $C_2$ concurrently issue \texttt{SetData("/bar", "A", -1)}
and \texttt{SetData("/bar", "B", -1)}.

\textbf{Observation:} After success, all clients read \texttt{GetData("/bar")} on $C_1$,
$C_2$, and follower $F$.

\textbf{Expected Result:} All nodes converge on a single final value in the leader's chosen
serialization order (either $A \rightarrow B$ or $B \rightarrow A$).
No interleaving or non-linearizable outcomes.

% \subsection{Monotonic Reads}
% \subsubsection{Test 3: Read-Only Client Monotonicity}
%
% \textbf{Setup:} Client $C_3$ reads from $F$. Leader $L$ performs updates.
%
% \textbf{Actions:}
% $L$: \texttt{SetData("/m", "1")} then \texttt{SetData("/m", "2")}.
% $C_3$ polls \texttt{GetData("/m)} every 100ms.
%
% \textbf{Expected Result:} Observed values are monotonic: once $C_3$ sees ``2'', it never
% returns ``1'' again. Temporary lag is allowed but regressions are not.

% \subsubsection{Test 4: Monotonic Reads Across Session Migration}
% 
% \textbf{Setup:} Client $C_4$ initially connected to follower $F_1$. Migrates mid-run to $F_2$.
% 
% \textbf{Actions:}
% $L$: \texttt{SetData("/n", "x")} then \texttt{SetData("/n", "y")}.
% $C_4$ reads \texttt{GetData("/n")} from $F_1$, migrates, then continues reading on $F_2$.
% 
% \textbf{Expected Result:} No backwards reads: after observing ``y'' on $F_1$, reads on 
% $F_2$ never return ``x''. Followers must not serve older states than those already observed.

% TODO
% \subsection{Cross-Node Consistency}
% \subsubsection{Test 5: Writes on Leader, Reads on All Nodes}
% 
% \textbf{Setup:} Five-node cluster. Clients $C_1\dots C5$ connected to different nodes.
% 
% \textbf{Actions:} Leader performs:
% \begin{enumerate}
%     \item \texttt{Create("/k"),"alpha"}.
%     \item \texttt{SetData("/k", "beta")}
% \end{enumerate}
% 
% \textbf{Observation:} All clients repeatedly read \texttt{GetData("/k")} for 5 seconds.
% 
% \textbf{Expected Result:} All nodes converge to ``beta''. No node returns a value outside the committed sequence.
% Temporary follower lag is acceptable.

\subsection{Leader Election: Killed Leader Causes Election}

\textbf{Setup:} Five-node cluster.

\textbf{Actions:}
\begin{enumerate}
    \item Start all five nodes.
    \item Wait for initial election to occur.
    \item Kill current leader node.
    \item Wait some time for election timeouts to occur.
    \item After that time has passed, there should be a new leader.
\end{enumerate}

\todo[inline]{observation, expected results}

\subsection{Leader Election: Restarting Enough Nodes Unblocks Election}

\textbf{Setup:} Five-node cluster.

\textbf{Actions:}
\begin{enumerate}
    \item Start all five nodes.
    \item Wait for initial election to occur.
    \item Kill current leader node and two additional follower nodes, leaving a total of $2$ of $5$ nodes alive.
    \item Wait some to see if a leader is elected.
    \item Restart one of the killed followers, bringing us back up to $3$ of $5$ nodes alive.
    \item Wait some additional time, and a leader should now actually be elected.
\end{enumerate}

\textbf{Observation:} On step 4, no leader should ever be elected. On step 6, a leader should be elected.

\textbf{Expected Result:} No leader is elected when there is not a majority alive.
Once there is a majority alive, there should eventually be a leader elected.


% \subsubsection{Failure Recovery}
% \subsubsection{Test 6: Leader Crash After Append, Before Commit}
% 
% \textbf{Setup:} Three-node cluster. Followers $F_1$, $F_2$.
% 
% \textbf{Actions:}
% Leader $L$ appends \texttt{SetData("/r", "new")} then crashes \emph{before} receiving a quorum ACK.
% 
% \textbf{Observation:} Reads on $F_1$ and $F_2$ via \texttt{GetData("/r)}.
% 
% \textbf{Expected Result:} Value remains the old value. The uncommitted write is discarded after new leader election. No 
% partial visibility.

% TODO
% \subsection{Quorum and Availability}
% \subsubsection{Test 7: Minority Partition Unavailable for Writes}
% 
% \textbf{Setup:} Five-node cluster partitioned into 3-node majority and 2-node minority.  
% 
% \textbf{Actions:} Clients attempt writes in both partitions.  
% 
% \textbf{Expected Result:}  
% \begin{itemize}
%   \item Majority partition continues serving writes normally.
%   \item Minority partition rejects or blocks writes.
%   \item Minority reads may lag or fail.
%   \item After healing, minority catches up from leader log.
% \end{itemize}

% TODO
% \subsubsection{Test 8: Node Join and Catch-Up}
% 
% \textbf{Setup:} Three nodes perform 100 sequential updates on \texttt{/z}.
% 
% \textbf{Actions:} Add new node $N_4$, allow it to synchronize.
% 
% \textbf{Expected Result:} After catch-up, $N_4$ returns the latest committed value.
% Log backfull converges without divergence.

% \subsection{Watches and Notifications}
% \subsubsection{Test 9: Watch Triggers on Immediate Children}
% AppendEntries Path (Follower \& Leader, \S5.3)
% \textbf{Setup:} Client registers a watch via \texttt{GetChildren("/p", watchChan)}.
% 
% \textbf{Actions:}
% Leader creates \texttt{/p/a}, then \texttt{/p/a/b}, then deletes \texttt{/p}.
% 
% \textbf{Expected Result:}
% Notifications fire for \texttt{/p/a} (child created) and \texttt{/p} (deleted).
% No notification for grandchild \texttt{/p/a/b}, consistent with ZooKeeper semantics.

\subsection{Operational Parameters}
\begin{itemize}
    \item Cluster sizes tested: 3-node and 5-node.
    \item Timing: heartbeat interval and election timeout ranges.
    \item Failure modes: crash-stop, network partitions.
    \item Client placement: leader-connected vs. \ follower-connected vs. \ migrating.
    \item Metrics recorded: commit index progression, read sequences, convergence time, 
    leadership transitions.
\end{itemize}
