\subsection{Success Scenarios}
In a successful system run, we define a cluster of three nodes running the MonkeyMinder service.
These nodes elect a single leader and begin servicing client requests. Then, a client connects to
one of the nodes and issues a series of read and write requests. Reads should be served locally
by the connected service, and writes should get forwarded to the leader. Upon receiving a quorum,
the leader responds to the server that originated the write request, who forwards the response back
to the client.

To demonstrate this, we create a simple client that performs the following sequence of service calls:
\begin{verbatim}
client.Create("/bar", "meow") # Should succeed
client.GetData("/bar", nil) # Should return "meow"
client.SetData("/bar", "bark", -1) # Should succeed
client.GetData("/bar", nil) # Should return "bark" 
client.Delete("/bar", -1) # Should succeed
client.GetData("/bar", nil) # Should return nil
\end{verbatim}

We start 3 servers from the `server/` subdirectory with the command `go run main.go --id n` for each n in {1,3}.
From the log outputs in \ref{fig:Leader Election}, we can see the three server nodes bind to ports 9001, 9002, 
and 9003 respectively. Furthermore, exactly one node (node 1 in the top right corner) achieves a quorum and 
thus is elected leader. Node 1 upon receiving a vote from node 2 achieves quorum, asserts itself as leader, 
and begins sending out heartbeats in the form of empty AppendEntries messages.

\easyfigure{screenshots/Election.png}{\label{fig:Leader Election}Electing a leader}

Next, we launch the client application in \ref{fig:Create}. The first thing the client does is try to create 
the node '/bar' with initial value 'meow'. Since the log is currently empty, this should succeed. Furthermore, 
because the client is connecting to node with `--id 2`, this Create request must be forwarded to the leader to 
ensure linearizability. This log shows that at 15:59:10, node 2 establishes a new client session. Behind the 
scenes, it receives the `Create` request and forwards it to the leader. Moving our attention to node 1 in the 
top right corner, at timestamp 15:59:10 node 1 (the leader) establishes a client session with node 1. Node 1
receives the forwarded request. Then, it quietly applies the entry to its own log and sends out AppendEntries
requests to its peers. We see at 15:59:11, node 2 is the first to receive the AppendEntries request that
contains the new state after the Create request. Upon receiving node 2's AppendEntries response, node 1
checks to see if it can commit the entry to the log. Since both node 1 and node 2 agree on the state of the
log up to index 1, node 1 commits the new entry by advancing the commit index to 1. Then, node 1 checks
if any watches have been set that trigger upon updates to the "/bar" znode. Indeed, when node 1 first
received the forwarded Create request, it registered a watch on the "/bar" znode. So the watch fires,
and node 1 sends a response to node 2 that the Create request succeeded. Node 2 forwards this result
to the client so it can move forward with its next command.

\easyfigure{screenshots/Create.png}{\label{fig:Create}Creating a znode}

After confirming that the znode has been created, the client wants to check the value of the newly created
znode at path "/bar". It sends out a `GetData` request to node 2. Because `GetData` is a read operation,
node 2 can respond to the client by just reading its local state. Node 2 quietly processes this request
which can be seen by client immediately receiving a value of 'meow'. Note also how no message was forwarded
to the leader, and no log entries were created from processing this read. This demonstrates that read
operations are extremely fast because they only require a single request-response pair between the
client and the server node that serves it.

\easyfigure{screenshots/Get.png}{\label{fig:GetData}Reading the znode's value with GetData}

The remaining figures demonstrate the successful use of each of our service APIs. \ref{\label{fig:SetData}}
demonstrates setting the value of a znode, which requires leader forwarding. \ref{\label{fig:Get Updated}}
verifies that the updated value is read by the client. \ref{\label{fig:Delete}} demonstrates that znodes
can be deleted. Finally, \label{fig:Get After Delete} shows that after a delete, a read on the node returns
a nil value.


\easyfigure{screenshots/Set.png}{\label{fig:SetData}Updating the znode's value with SetData}
\easyfigure{screenshots/GetUpdated.png}{\label{fig:Get Updated}Reading the updated value}
\easyfigure{screenshots/Delete.png}{\label{fig:Delete}Deleting the znode}
\easyfigure{screenshots/Get_Deleted.png}{\label{fig:Get After Delete}Reading the znode after deletion}

\subsection{Failure Scenarios}
Our failure model for the system involves the failure of individual nodes in the system. We divide this
model into three different scenarios: follower failure, leader failure, and node recovery. 

\subsubsection{Follower Failure}
The first scenario is protected against both quorum model requirement. Raft guarantees that as long 
as a quorum of nodes in the cluster are available, the service remains available and live. This is 
because both the leader election protocol and the protocl for committing updates to the log. In
order for a candidate to assert themselves as leader, they must receive votes from a quorum of
the cluster. Likewise, in order to permanently commit a new entry to the log, the leader must
receive acknowledgement from a quorum of nodes that they applied the entry via the AppendEntries
RPC.

In figure \ref{fig:Follower Death 1}, we see a healthy cluster with node 3 as leader and nodes
1 and 2 as followers. In \ref{fig:Follower Death 2}, the test script kills node 1. Finally,
in \ref{fig:Follower Death 3} the test script issues a write to node 2, which gets replicated
across the cluster. This shows how even when nodes fail in the cluster, liveness is still
guaranteed as long as a quorum remains alive.

\easyfigure{screenshots/failure_follower_death-1.png}{\label{fig:Follower Death 1}Failure scenario 1: A healthy cluster}
\easyfigure{screenshots/failure_follower_death-2.png}{\label{fig:Follower Death 2}Failure scenario 1: A follower dies}
\easyfigure{screenshots/failure_follower_death-3.png}{\label{fig:Follower Death 3}Failure scenario 1: The cluster continues serving clients}

\subsubsection{Leader Failure}
The second scenario is protected against by Raft's election protocol and Leader Completeness
property. The Leader Completeness property states "if a log entry is committed in a given term, then 
that entry will be present in the logs of the leaders for all higher-numbered terms" 
\cite{ongaro_search_2014}. When a leader fails, one of the followers will eventually discover this 
failure via the election timeout mechanism. We can see in \ref{fig:Leader Death 1} that the cluster 
starts in a healthy state with node term 2 and the leader having a single committed entry. The 
follower nodes both replicate the entry, but have not yet committed it to the state machine. In 
\ref{fig:Leader Death 2} the leader crashes, and the cluster recovers by electing node 2 as leader. 
Because the log entry was committed by the leader in term 2, and starting an election increments
the term, this guarantees that the new leader will have the entry in its log. Finally, we again see
in \ref{fig:Leader Death 3} that after recovering from the leader failure, the service remains
available to process more write requests.

\easyfigure{screenshots/failure_leader_death-1.png}{\label{fig:Leader Death 1}Failure scenario 2: A healthy cluster}
\easyfigure{screenshots/failure_leader_death-2.png}{\label{fig:Leader Death 2}Failure scenario 2: The leader dies; a new leader is elected}
\easyfigure{screenshots/failure_leader_death-3.png}{\label{fig:Leader Death 3}Failure scenario 2: The cluster continues serving clients}

\subsubsection{Node Recovery}
The third scenario our system handles is the recovery of nodes after failure. When a node wants to
rejoin the cluster, it needs to be able to catch up with the leader's logs. Periodically, the leader
will send out heartbeats to all nodes in the cluster via AppendEntries messages. These messages
serve two purposes: they act as a failure detector for detecting when a leader fails, and they
enable the leader to propagate its log to the rest of the cluster. Through the `prevLogIndex` and 
`prevLogTerm` parameters, the leader negotiates with its followers to figure out where the their
logs diverge and which entries it needs to send for the recovering node to match its log to the
leader's. 

In \label{fig:Node Recovery 1}, we see an initial state with node 1 as leader, node 2
dead, and node 3 as follower. The cluster agrees on four entries being committed to the log.
Upon restarting node 2 in \label{fig:Node Recovery 2}, node 2 experiences an election timeout
before it can receive any heartbeats from the leader. However, because both its term and
log is behind the other two nodes, it fails to acqurie any votes. In \label{fig:Node Recovery 3},
node 2 finally receives the AppendEntries message from the leader which contains all four of the
committed entries. Because a quorum of nodes remained active, the recovering node 2 was able
to rejoin the cluster and update its log to the current state of the leader.

\easyfigure{screenshots/failure_node_recovery-1.png}{\label{fig:Node Recovery 1}Failure scenario 3: A node is dead}
\easyfigure{screenshots/failure_node_recovery-2.png}{\label{fig:Node Recovery 2}Failure scenario 3: The node rejoins the cluster}
\easyfigure{screenshots/failure_node_recovery-3.png}{\label{fig:Node Recovery 3}Failure scenario 3: The recovered node catches up to the leader's log}
